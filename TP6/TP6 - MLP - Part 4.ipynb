{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIjFqlgERHAq"
      },
      "source": [
        "# IMA 205 - TP ANN (part C) - OPTIONAL\n",
        "\n",
        "## Coding a Multi-Layer Perceptron with Keras/Tensorflow\n",
        "\n",
        "This part is optional, and you do not have to put your solution on eCampus. Regardless, I encourage you to do this part if you have spare time during the lab as it gives you a chance to familiarize yourself with the higher-level API of Keras. You will see that when the task is straightforward and involves only pre-existing building blocks, implementing it via Keras is very fast."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Original Author : Alasdair Newson\n",
        "\n",
        "Adapted by Loic Le Folgoc. If you have questions, you can contact me at loic.lefolgoc@telecom-paris.fr"
      ],
      "metadata": {
        "id": "feyuXrYp6oaM"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oG_lwX5GRHAs"
      },
      "source": [
        "In the second part of this TP, we will be looking at the same Multi-Layer Perceptron (MLP) practical work, using Keras (https://keras.io/)\n",
        "\n",
        "We will be using the following packages :\n",
        "   \n",
        "   - Scikit-learn (http://scikit-learn.org/)\n",
        "   - Keras (https://keras.io/)\n",
        "\n",
        "The following commands will make sure that you have all the necessary packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sCGFw05kRHAs"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import pdb\n",
        "\n",
        "import sklearn  # scikit-learn\n",
        "import keras\n",
        "\n",
        "from keras import layers\n",
        "print(keras.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This is the backend used by Keras by default:\n",
        "keras.backend.backend()"
      ],
      "metadata": {
        "id": "CA7n06PiC0yr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxgAcMrMRHAt"
      },
      "source": [
        "# 1 - Multi-Layer Perceptron with Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjK-B5mkRHAt"
      },
      "source": [
        "## Introduction to Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzVmdesCRHAu"
      },
      "source": [
        "Keras is a high-level, deep learning API originally written by Fran√ßois Chollet and now developed by Google for implementing neural networks. Like Pytorch, it provides automatic differentiation so that the user does not have to determine gradients manually, which can be extremely long even for simple networks, as you have seen in the previous lab.\n",
        "\n",
        "Previously, Keras was a high-level API using Tensorflow as a backend. Tensorflow is a library written by Google which also allows implementation of deep neural networks. Previously, Tensorflow required a strict separation between the creation (declaration) of variables and their execution (giving a numerical value, using the ```Session``` function). With Keras and Tensorflow 2, this separation has been removed, and the language has become simpler, in particular for creating and training networks.\n",
        "\n",
        "Since Keras 3, Keras can now be used on top of any backend in a transparent manner: JAX, Pytorch, or Tensorflow. By default, it still uses Tensorflow (as in our code below): but you will notice the code itself makes no explicit reference to Tensorflow.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating a network\n",
        "\n",
        "To create a network, there are two (main) methods :"
      ],
      "metadata": {
        "id": "oo5mY5E1fYqj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating a model using the Sequential API\n",
        "\n",
        "Keras has a simple way of adding layers to create a neural network. First, you can indicate to Keras that the model is 'sequential' (a simple model, with not many tweaks). For this, you can use the following function :\n",
        "- ```model = keras.Sequential()```\n",
        "\n",
        "After this, you can add layers with the function.\n",
        "\n",
        "- ```model.add()```\n",
        "\n",
        "You can then use the ```layers.Dense``` and ```Activation``` functions to specify different layer types.  __Note that in the case of this approach, you will have to specify the input image size in the first layer of the network, inside the first layer function.__ So, for example, if the first layer is a dense layer with a relu activation, with n_out output neurons, and n_input neurons :\n",
        "\n",
        "- `model = keras.Sequential()`\n",
        "- `model.add(layers.Dense(n_out,activation,input_shape=(n_in,),activation='relu')`\n",
        "\n",
        "Otherwise, the network does not know how many weights to create. __Be careful of this special case (the first layer)__.\n",
        "\n",
        "Notice that here, the activation is simply an argument of the `Dense` layer.<br>\n",
        "\n",
        "Alternatively, like in Pytorch, we can pass the layers as arguments to `Sequential`: `model = keras.Sequential([layers.Dense(...),layers.Dense(...)])`."
      ],
      "metadata": {
        "id": "XGeaBLzwfecj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating a model using the standard API\n",
        "\n",
        "Otherwise, another approach to creating the model is to explicitly create the input variable, and just cascade the different functions, as in Tensorflow. So, for the same example, we would have :\n",
        "\n",
        "- `input = keras.Input(shape=(n_in,))`\n",
        "- `output = layers.Dense(n_in,n_out,activation='relu')(input)`\n",
        "- `model = keras.Model(input, output)`\n",
        "\n",
        "\n",
        "For now, let's use the Sequential API (however, if you want to try the standard API, go ahead)."
      ],
      "metadata": {
        "id": "gbFP6ViBfjRM"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYFxzt5mRHAy"
      },
      "source": [
        "# MLP with Keras : Architecture\n",
        "\n",
        "We are now going to create an MLP with Keras. We will start with an MLP with one hidden layer. The network should consist of the following operations, in the following order :\n",
        "\n",
        "- Fully connected layer, with 50 output neurons\n",
        "- ReLU activation\n",
        "- Fully connected layer, with 1 output neuron (because we have a binary classification problem)\n",
        "- Sigmoid output activation\n",
        "\n",
        "We are going to be solving a binary classification problem, so the output of the network should be a scalar between 0 and 1 (thus the last Sigmoid activation function).\n",
        "\n",
        "The loss function should be defined as the binary cross-entropy between the predicted class and the true class"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training and testing a network\n",
        "\n",
        "Tensorflow allows the easy training of a network with the following functions :\n",
        "\n",
        "- `model.compile(loss=keras.losses.binary_crossentropy, optimizer=keras.optimizers.Adam(learning_rate=learning_rate))` : create the loss function and define the optimizer used. This function can do many other useful things (such as specifying different metrics to look at the model's performance e.g., with the argument: `metrics=[\"accuracy\"]`)\n",
        "- `model.fit(...)` : train the model\n",
        "- `model.evaluate(...)` : test the model\n",
        "- `model.predict(...)` : carry out a simple forward pass on the model"
      ],
      "metadata": {
        "id": "ZVDHVAOeftPe"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdGVzE6NRHAy"
      },
      "source": [
        "# Load data\n",
        "\n",
        "First, we load the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EfiJwuLmRHAy"
      },
      "outputs": [],
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X, Y = datasets.make_circles(n_samples=1000, noise=0.05, factor=0.5)\n",
        "X = MinMaxScaler().fit_transform(X)\n",
        "\n",
        "#show data in plot\n",
        "plt.plot(X[Y == 1, 0], X[Y == 1, 1], 'ro')\n",
        "plt.plot(X[Y == 0, 0], X[Y == 0, 1], 'bo')\n",
        "plt.grid('on')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRL1c6UyRHAy"
      },
      "source": [
        "We are now going to define some useful auxillary functions.\n",
        "\n",
        "First, a function that shows the decision boundary of our network. This works only for 2D input data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x0cUhY8xRHAz"
      },
      "outputs": [],
      "source": [
        "# THIS CODE IS GIVEN\n",
        "\n",
        "def plot_decision_function_2d(model_mlp, X, Y):\n",
        "   # create a mesh to plot in\n",
        "    h = .02  # step size in the mesh\n",
        "    offset = 0.1\n",
        "    x_min, x_max = X[:, 0].min() - offset, X[:, 0].max() + offset\n",
        "    y_min, y_max = X[:, 1].min() - offset, X[:, 1].max() + offset\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                         np.arange(y_min, y_max, h))\n",
        "\n",
        "    # Plot the decision boundary. For that, we will assign a color to each\n",
        "    # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
        "    Z = model_mlp.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "\n",
        "    Z = Z<0.5\n",
        "    # Put the result into a color plot\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    plt.figure()\n",
        "    plt.contourf(xx, yy, Z, cmap=plt.cm.Paired)\n",
        "\n",
        "    plt.plot(X[Y == 1, 0], X[Y == 1, 1], 'yo')\n",
        "    plt.plot(X[Y == 0, 0], X[Y == 0, 1], 'ko')\n",
        "\n",
        "    plt.title(\"Decision surface\")\n",
        "    plt.axis('tight')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKtZJ5iQRHA0"
      },
      "source": [
        "## CREATING AND TRAINING THE MODEL\n",
        "\n",
        "We are now ready to create our network with Keras and to carry out training on our training dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WO8dHtrGRHA0"
      },
      "outputs": [],
      "source": [
        "# FILL IN CODE BY STUDENTS IN THIS SECTION\n",
        "\n",
        "# We split up the data into training and test data, using a function from Scikit-learn :\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.15)\n",
        "print(X_train.shape)\n",
        "\n",
        "# Layer sizes\n",
        "n_input = X.shape[1]  # Number of input features\n",
        "n_hidden = 50  # Number of neurons in the hidden layer\n",
        "n_output = 1 # Number of output neurons\n",
        "\n",
        "# create the model (using the sequential API)\n",
        "\n",
        "model_mlp = keras.Sequential([...])   # FILL IN STUDENTS\n",
        "\n",
        "# create the loss and optimiser\n",
        "learning_rate = 0.01\n",
        "model_mlp.compile(loss=..., optimizer=...,metrics=[\"accuracy\"]) # TO FILL IN\n",
        "\n",
        "# Run optimisation algorithm\n",
        "n_epochs = 30\n",
        "batch_size = 64\n",
        "\n",
        "print('Training')\n",
        "model_mlp.fit(..., ..., epochs=...,batch_size=...) # TO FILL IN\n",
        "\n",
        "print('Testing')\n",
        "model_mlp.evaluate(...,  ..., verbose=2) # TO FILL IN\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lpBaU1rcc0oY"
      },
      "outputs": [],
      "source": [
        "plot_decision_function_2d(model_mlp, X_test, Y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6oni6Q5vRHA0"
      },
      "source": [
        "# 2 - MLP with several hidden layers, on MNIST\n",
        "\n",
        "We are now going to create an MLP with several hidden layers. We are going to use a more complicated dataset : the MNIST dataset, which contains images of handwritten digits. There are 10 classes, one for each digit.\n",
        "\n",
        "We are going to implement the following architecture :\n",
        "\n",
        "- Fully connected layer\n",
        "- Relu activation\n",
        "- Fully connected layer\n",
        "- Relu activation\n",
        "- Fully connected layer\n",
        "- Relu activation\n",
        "- Fully connected layer\n",
        "- Softmax output activation\n",
        "\n",
        "__IMPORTANT Note__ The sigmoid layer has been replaced by a softmax layer (at the end). This is normal, since we have a multi-class problem. __However, this differs from our Pytorch implementation, because Pytorch includes the softmax in the cross-entropy loss, unlike Keras.__\n",
        "\n",
        "\n",
        "First, we load the MNIST dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z0cGLMKRRHA0"
      },
      "outputs": [],
      "source": [
        "# Import MNIST data\n",
        "mnist = keras.datasets.mnist\n",
        "(X_train, y_train),(X_test, y_test) = mnist.load_data()\n",
        "\n",
        "Y_train = keras.utils.to_categorical(y_train)  # in order to convert y to a matrix with (num_examples, num_classes) (one-hot encoding)\n",
        "Y_test = keras.utils.to_categorical(y_test)  # in order to convert y to a matrix with (num_examples, num_classes) (one-hot encoding)\n",
        "\n",
        "#reshape the input images : flatten the last two dimensions\n",
        "X_train = np.reshape(X_train,(X_train.shape[0],X_train.shape[1]*X_train.shape[2]))\n",
        "X_test = np.reshape(X_test,(X_test.shape[0],X_test.shape[1]*X_test.shape[2]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzMV_9-NRHA1"
      },
      "source": [
        "Finally, fill in the following code to create and train your MLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2G7AToqyRHA1"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Network Parameters\n",
        "n_hidden_1 = 256 # 1st layer number of neurons\n",
        "n_hidden_2 = 256 # 2nd layer number of neurons\n",
        "n_hidden_3 = 128 # 3rd layer number of neurons\n",
        "\n",
        "n_input = X_train.shape[1]\n",
        "n_classes = Y_train.shape[1] # MNIST total classes (0-9 digits)\n",
        "\n",
        "# TO CODE BY STUDENTS\n",
        "\n",
        "\n",
        "model_mlp_multi_layer = ...\n",
        "\n",
        "# create the loss and optimiser, use keras.losses.categorical_crossentropy in loss\n",
        "...\n",
        "\n",
        "# Run optimisation algorithm\n",
        "n_epochs = 20\n",
        "batch_size = 64\n",
        "\n",
        "print('Training')\n",
        "...\n",
        "\n",
        "print('Testing')\n",
        "..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6Q-JntxluxG"
      },
      "outputs": [],
      "source": [
        "# THIS CODE IS GIVEN\n",
        "\n",
        "def test_mnist_images(model_mlp_multi_layer,X_test):\n",
        "  plt.figure(figsize=(10, 6))\n",
        "  for idx in range(0,10):\n",
        "      plt.subplot(2, 5, idx+1)\n",
        "      rand_ind = np.random.randint(0,X_test.shape[0])\n",
        "      plt.imshow(np.reshape(X_test[rand_ind,:],(28,28)),cmap='gray')\n",
        "      # get prediction\n",
        "      model_prediction = np.argmax(model_mlp_multi_layer.predict(np.expand_dims( X_test[rand_ind,:], axis=0)),axis=1)\n",
        "      plt.title(int(model_prediction))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r2D9RE4apVWW"
      },
      "outputs": [],
      "source": [
        "test_mnist_images(model_mlp_multi_layer,X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lItdBVnfu7xK"
      },
      "source": [
        "You may observe some difficulties in getting good performance in this case. For me, this was much more obvious here than in the Pytorch implementation for some reason (maybe different implementation choices: in the initialization of the layers, the optimizer, etc.). To improve training, we can turn to __regularisation__."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vgao_U5XRHA2"
      },
      "source": [
        "## BATCH NORMALISATION\n",
        "\n",
        "One approach to improving the convergence of neural network training which we have seen during the lesson is known as batch normalisation, which we have seen during the lesson. This can be implemented very simply in Tensorflow by adding the following layer :\n",
        "\n",
        "- ```layers.BatchNormalization()```\n",
        "\n",
        "Change your model below, and implement this using your code above"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Ela5-4XRHA2"
      },
      "outputs": [],
      "source": [
        "# TO CODE BY STUDENTS\n",
        "\n",
        "..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cSxGzvQ3pg3u"
      },
      "outputs": [],
      "source": [
        "test_mnist_images(model_mlp_multi_layer,X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Question__: do you observe improved convergence ?"
      ],
      "metadata": {
        "id": "pLNi42wMiCXC"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQPJDop31Nci"
      },
      "source": [
        "## Dropout\n",
        "\n",
        "Often in the process of training neural networks, there is a difference between the training accuracy and the testing accuracy. This is the problem of overfitting. To alleviate this problem, we can turn to dropout:\n",
        "\n",
        "`layers.Dropout(rate,...)`\n",
        "\n",
        "where rate is the probability that an input neuron to a layer will get set to 0. You can do this in any layer. Use the previous architecture and try out the dropout. Set the dropout rate to 0.1 on the first layer as a test. Then try it out on all layers (you may not see a great difference here since the mnist database is relatively simple, this is just to show you how it is done)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-BF-fpo1PpW"
      },
      "outputs": [],
      "source": [
        "dropout_rate = 0.1\n",
        "\n",
        "# TO CODE BY STUDENTS\n",
        "\n",
        "...\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2CWgKZeRHA2"
      },
      "source": [
        "#### Documentation:\n",
        "\n",
        "  - https://keras.io/\n",
        "  - http://www.deeplearningbook.org/"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}